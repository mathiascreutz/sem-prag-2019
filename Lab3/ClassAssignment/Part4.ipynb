{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 3, Part 4\n",
    "\n",
    "---\n",
    "\n",
    "### Section 3.1: Sentence embeddings\n",
    "\n",
    "In this part we will take a quick look at phrase/sentence embeddings. In the last exercise in Part 3 you figured out how to average vectors. As is turns out, averaging the embeddings of the words in a phrase is also one of the simplest ways of creating an embedding for the phrase. \n",
    "\n",
    "For example, let's imagine we have four words with the following embeddings:\n",
    "\n",
    "    the       = [1, 1, 2, 1]\n",
    "    cat       = [2, 0, 1, 3]\n",
    "    is        = [1, 0, 2, 0]\n",
    "    beautiful = [4, 1, 0, 0]\n",
    "    \n",
    "The embedding for the phrase *the cat is beautiful* could be calculated by averaging the four vectors:\n",
    "\n",
    "    (the + cat + is + beautiful)/4 = [2, 0.5, 0.75, 1]\n",
    "    \n",
    "There are a multitude of ways to improve on this simple baseline, but given good word embeddings trained on a very large corpus, this method works surprisingly well on many different tasks. \n",
    "\n",
    "In the code cell below you can try out visualizing embeddings for different sentences. The intuition that guided our thinking with word vectors works here too: Similar sentences should have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_utils\n",
    "%matplotlib notebook\n",
    "\n",
    "embeddings, mapping = plot_utils.get_embeddings()\n",
    "\n",
    "sents = [\"the man saw the queen\",\n",
    "         \"the king kissed the queen\",\n",
    "         \"the boat was really fast\",\n",
    "         \"the dog ran very fast\",\n",
    "         \"the princess was trying to hug the prince\",\n",
    "         \"the dog chased the cat\",\n",
    "         \"the royal family moved to a new palace\",\n",
    "         \"the animals were running around\",\n",
    "         \"no bear has ever been seen in finland\"]\n",
    "\n",
    "plot_utils.plot_sentences_2d(sents, embeddings, mapping)\n",
    "plot_utils.plot_sentences_3d(sents, embeddings, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 3.1.1** **This is an advanced exercise, don't worry if you can't make sense of it at all.** \n",
    "\n",
    "Implement an alternative function for creating sentence embeddings. The function should take three arguments: `sentence`, which is the sentence to be embedded as a string; `embeddings`, a NumPy matrix (2d array) which contains the word embeddings; `mapping`, which contains the mapping from words to their row indices in `embeddings`.\n",
    "\n",
    "One improvement to the basic word averaging model would be to use weighted averaging, where some words contribute more to the final embedding than others. For this you will probably need some additional resources (like a frequency list for English words).\n",
    "\n",
    "You can test your function by supplying it to the `plot_utils.plot_sentences_Nd` functions. This can be done via an argument `embedding_fn`:\n",
    "\n",
    "    def some_fn(args): return 0\n",
    "    \n",
    "    plot_utils.plot_sentences_2d(sents, embeddings, mapping, embedding_fn=some_fn)\n",
    "    \n",
    "*Note: You can take a look at how the word-averaging model is implemented efficiently by navigating to a directory called `src` in the home directory (where you landed when you first opened this instance). The function is `embed_sentence` and can be found in the `plot_utils.py` file. It might also contain some hints for the weighted averaging.*\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_embedding(sentence, word_embeddings, mapping):\n",
    "    \"\"\"Creates a sentence embedding using some nice function.\"\"\"\n",
    "    return np.zeros(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now continue to the home assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
