{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 3, Part 2\n",
    "\n",
    "---\n",
    "\n",
    "In Part 2 of this session we will move on from binary hand-crafted features to the current state of the art word embeddings. More specifically, the embeddings we will use are learned with one variant of a group of models often called **Word2Vec** (W2V). Unfortunately the model itself is again a bit too complicated for this course.\n",
    "\n",
    "In two sentences, the W2V model we use works by constructing an artificial task on a very large corpus of text. In the task we train a neural network to predict which words occur in the context of a given word, and magically, awesome word embeddings emerge. \n",
    "\n",
    "*The mathematically inclined can read more about Word2Vec for example [here](https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b).*\n",
    "\n",
    "As always, import the necessary library by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook",
    "\n",
    "import plot_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: Visualizing Word2Vec embeddings\n",
    "\n",
    "First let's retrieve the embeddings we want to use. `plot_utils` offers a convenient function `get_embeddings`, which returns the embeddings and a dictionary that we call `mappings`. Run the cell. The output and the variables `embeddings` and `mapping` are explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, mapping = plot_utils.get_embeddings()\n",
    "\n",
    "print(\"'embeddings' is a matrix with dimensions %s.\" % str(embeddings.shape))\n",
    "print(\"The number of vocabulary items is %d.\" % len(mapping))\n",
    "print(\"The dimensionality of the embeddings is %d.\" % embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings are gathered in a matrix where each row corresponds to a vocabulary item and each column is a feature. Our matrix has dimensions `(25707, 300)`, that is, the number of words is 25707 and each word is represented as a 300-dimensional vector.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c c}\n",
    "\\textbf{rows are words} & \\textbf{columns are features}\\\\\n",
    "\\downarrow & \\downarrow \\\\\n",
    "  \\begin{array}{c c c}\n",
    "  you \\\\\n",
    "  dog \\\\\n",
    "  cat \\\\\n",
    "  ... \\\\\n",
    "  jump\n",
    "  \\end{array} \n",
    "&\n",
    "\\left[\n",
    "  \\begin{array}{c c c c}\n",
    "  0.1   & 0.001 & ... & 0.032 \\\\\n",
    "  0.23  & 0.062 & ... & 0.02 \\\\ \n",
    "  \\textbf{0.57}  & \\textbf{0.042} & ... & \\textbf{0.02} \\\\ \n",
    "        &       &     &      \\\\ \n",
    "  0.012 & 0.4   & ... & 0.091\n",
    "  \\end{array}\n",
    "\\right] \n",
    "& \\leftarrow \\textbf{This row is the vector for \"cat\"}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In contrast to the vectors in part 1, the features (dimensions) in these vectors don't necessarily have a clear interpretation. We cannot always say that, for example, feature number 152 indicates 'adultness', or any other intuitive semantic feature. What matters is that similar words should have similar vectors.\n",
    "\n",
    "So what is the variable `mapping`? It is a dictionary of `(word, index)` pairs, a mapping from words to their row indices in the embedding matrix. For example, the first row of the matrix `embeddings` is the vector for the word \"*you*\", so the value returned by `mapping[\"you\"]` is `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mapping[\"you\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the vector for \"*you*\" using the familiar indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prints out a 300-dimensional vector that can be annoyingly\n",
    "# large in the output cell. You can collapse the output by clicking\n",
    "# the left margin between the output and the colored bar on the left.\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, you don't actually need to manipulate or retrieve the embeddings yourself, but it is good to know how they are handled in Python. Any manipulation of the embeddings is done by functions, to which we supply the embeddings and mappings as arguments.\n",
    "\n",
    "We will again visualize the embeddings. In the code cell below we supply a list of words and illustrate the usage of two new functions `plot_w2v_2d` and `plot_w2v_3d` for plotting the W2V vectors. Notice that you need to supply the embeddings and the mapping dictionary in addition to the word list.\n",
    "\n",
    "Run the code cell below and inspect the output. Does it make any sense? (If the plots are not dynamic, rerun the cell at the top of this page, and then the cell below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = \"dog cat king queen man woman prince house car motorcycle pig horse cow\".split()\n",
    "\n",
    "plot_utils.plot_w2v_2d(\n",
    "    words=word_list,\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    arrows=True\n",
    ")\n",
    "\n",
    "plot_utils.plot_w2v_3d(\n",
    "    words=word_list,\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    arrows=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1.2.1** In the code cell below, analyze and visualize the embeddings for some words of you choice. Try to see if you can find patters in the results: Which word pairs/groups seem to work well and which don't? \n",
    "\n",
    "Use a reasonable number of words (for example 6 or more).\n",
    "\n",
    "All words should be lowercase. If you use the `split` method as above, the words need to be separated by whitespace. You can try out visualizing in 2D and 3D, with and without arrows, depending on which style you like.\n",
    "\n",
    "If you can't see a word in the output, you might have chosen a word that is not in the vocabulary. The vocabulary contains a bit less than 26000 frequent English words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_list = \"word1 word2\".split()\n",
    "\n",
    "plot_utils.plot_w2v_3d(\n",
    "    words=word_list,\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    arrows=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Vector arithmetic with Word2Vec\n",
    "\n",
    "In the slides we saw some interesting properties of W2V- and GloVe-embeddings under the heading *Compositional meaning in Word2Vec and GloVe*. The examples show how the relations between the word embeddings in vector space match our intuitions; for example, subtracting the embedding for *man* from *king* and adding *woman* yield *queen* (in the optimal case).\n",
    "\n",
    "    king - man + woman = queen\n",
    "\n",
    "In this section we will try to visualize this process.\n",
    "\n",
    "Run the code cell below. You will see a single vector for the word *king*. Now change the function argument `minus` to `\"man\"` instead of `None`. You should see two red vectors. One endpoint is labeled *man*: this is the original word embedding. You might notice that the second red vector points exactly to the opposite direction. This is the vector `-man`. If you now follow the vectors for `king` and `-man`, you will end up at the point for `king - man`.\n",
    "\n",
    "Now change the argument `plus` to `\"woman\"`. You should see three new vectors: two blue and one yellow. The blue vectors are both the vector for `woman`, simply starting from different points. The yellow vector is the final result:\n",
    "\n",
    "    king - man + woman\n",
    "    \n",
    "Change the argument `result` to `\"queen\"`, which is what the result should intuitively be. Is the yellow vector anywhere near the point labeled *queen*?\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2.2.1** Try out vector arithmetic on some words of your own choice. Try to figure out equations of the following form:\n",
    "\n",
    "    king - man + woman = queen\n",
    "    paris - france + germany = berlin\n",
    "\n",
    "Do the result make any sense? Evaluating these things is hard when all you have is a figure. In the next lab session we will actually calculate vector similarities and distances, so that we can get more rigorous answers to these kinds of questions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import plot_utils\n",
    "\n",
    "embeddings, mapping = plot_utils.get_embeddings()\n",
    "\n",
    "plot_utils.plot_w2v_algebra(\n",
    "    embeddings=embeddings,\n",
    "    mapping=mapping,\n",
    "    base=\"king\",\n",
    "    minus=None,\n",
    "    plus=None,\n",
    "    result=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now move on to Part 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
