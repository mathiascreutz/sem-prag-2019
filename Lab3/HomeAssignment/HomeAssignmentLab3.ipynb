{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 home assignment: Distributional semantics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import distribsem\n",
    "import numpy as np\n",
    "\n",
    "from nltk.book import text1 as moby_dick, text2 as sense_and_sensibility\n",
    "\n",
    "# These lines filter out some characters from the texts to make it less noisy\n",
    "moby_dick = distribsem.filter_text(moby_dick)\n",
    "sense_and_sensibility = distribsem.filter_text(sense_and_sensibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "*You can get a maximum of 1 point from this task.*\n",
    "\n",
    "In this first task you will perform basic calculations with vectors similar to what we did in the Part 3 of the class material. **Use NumPy arrays for all of the vectors in this assignment!**\n",
    "\n",
    "In the code cell below, solve the following four subtasks:\n",
    "\n",
    "1. Create two vectors of length 10 where each element is the number 1.0. Add the vectors together and print out the result. *Hint: np.ones*\n",
    "\n",
    "2. Create two vectors of length 5 where each element is the number 1.0, then multiply one of the vectors by a scalar 2 and the other vector with a scalar 3. Add the vectors together and print out the result.\n",
    "\n",
    "3. Create two vectors `a` and `b` without using `np.ones` or `np.zeros` so that when `b` is subtracted from `a`, the result is the vector: \n",
    "    \n",
    "    `[1, 1, 2, 3, 5, 8]`\n",
    "    \n",
    "    You can choose your two vectors freely. Print out the vectors as well as the subtraction in the program.\n",
    "\n",
    "4. Calculate and print out the weighted average of the following vectors:\n",
    "    \n",
    "    `v1 = [1, 3, 5]     v2 = [1, 4, 7]     v3 = [1, 5, 8]`\n",
    "    \n",
    "    The weights should be `2`, `3` and `1` for `v1`, `v2`, and `v3` respectively.\n",
    "    \n",
    "    *Hint: To calculate a weighted average, instead of dividing by the number of vectors like in regular averaging, you need to divide by the sum of the weights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# TODO\n",
    "\n",
    "\n",
    "# 2\n",
    "# TODO\n",
    "\n",
    "\n",
    "# 3\n",
    "# TODO\n",
    "\n",
    "\n",
    "# 4\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "*You can get a maximum of 1.5 points from this task.*\n",
    "\n",
    "Your second task is to study the contexts of different words, keeping in mind the ideas about learning word embeddings from corpora. You can read slides 21 - 29 from Lecture 2 if you need a refresher on this. We also remind you of some basics here.\n",
    "\n",
    "When learning word embeddings from a corpus, there are a few parameters we need to decide on beforehand. First of all, we need to define a window size for the context we want to use. For example, a window of 4 words means we will consider as context the four words before and the four words after the target word.\n",
    "\n",
    "We also need to decide the dimensionality of the final vectors. In our case this means the size of the context vocabulary we want to take into account. We can decide, for example, to use the 1000 most common words in the corpus as context vocabulary. Consequently, our embeddings will be 1000-dimensional. Any words outside the top 1000 will simply not be taken into account.\n",
    "\n",
    "In the code cell below, you are given a function `show_kwic` (kwic = **key-word in context**), that you can use to retrieve instances of a word of you choice in a corpus. The example shows the 10 first occurrences of the word *water* in *Moby Dick*, using a window size of 4 and 500 most common words as context vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "**Your task:** Pick **one pair** of *closely related* words (for example *sky* and *cloud* can be considered closely related) and **one pair** of *unrelated* words (for example *house* and *jump*). Study the contexts of the words and see how/whether the (un)relatedness is manifested in the contexts. Answer as comments in the code cell.\n",
    "\n",
    "You should also try different values for `dimensionality` and `window_size`. `dimensionality` controls the size of the context vocabulary (using top frequent words) and `window_size` controls the window size of the context. Try `dimensionality` values in different ranges (tens, hundreds, thousands) and vary the window size for example between 1 and 10. Write as comments in the code cell answers to the following questions:\n",
    "\n",
    "1. How does dimensionality affect how easy it is to see the (un)relatedness of the words?\n",
    "2. How does window size affect how easy it is to see the (un)relatedness of the words?\n",
    "3. What kinds of characteristics do you think the embeddings capture with small window sizes and dimensionalities?\n",
    "3. What kinds of characteristics do you think the embeddings capture with large window sizes and dimensionalities?\n",
    "\n",
    "You can use either the text `moby_dick` or `sense_and_sensibility`.\n",
    "\n",
    "**Note:** *Words outside the top \"dimensionality\" frequent words are replaced with UNKs in the kwics.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dimensionality = 500\n",
    "window_size = 4\n",
    "show_n_occurrences = 10\n",
    "word = \"water\"\n",
    "\n",
    "distribsem.show_kwic(\n",
    "    text=moby_dick,\n",
    "    word=word,\n",
    "    window=window_size,\n",
    "    dimensionality=dimensionality,\n",
    "    show_n=show_n_occurrences\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "*You can get a maximum of 1.5 points from this task.*\n",
    "\n",
    "In this third task we will continue the theme of word embeddings and contexts. This time we will study embeddings trained on two different texts. The texts we will use are the familiar *Moby Dick* (variable `moby_dick`) and *Sense and Sensibility* (variable `sense_and_sensibility`). \n",
    "\n",
    "In the code cell below we train embeddings for a set of words on both texts (function `create_vectors_shared`). This will result in two embedding matrices (`M_moby_shared` and `M_sense_shared`) as well as a mapping from words to their row indices (`mapping_shared`). The vocabulary size and embedding dimensionalities are a bit weird because of the way they are restricted to get comparable embeddings. Don't worry about the fact that the values don't match the ones supplied to the function.\n",
    "\n",
    "Run the code cell and read on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_moby_shared, M_sense_shared, mapping_shared = distribsem.create_vectors_shared(\n",
    "    max_vocab_size=10000,\n",
    "    min_dimensionality=1000,\n",
    "    window_size=4,\n",
    "    text1=moby_dick,\n",
    "    text2=sense_and_sensibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below we show how you can visualize the word embeddings. Embeddings trained on different texts are color-coded. \n",
    "\n",
    "**3.1: Your first task** is to find **two** words that have relatively similar embeddings in the two texts (meaning the two embeddings for *one word* are close to each other in the figure) as well as **two** words that have completely different embeddings (again, cases where the same word gets two very different embeddings in the two texts). Plot the four words like in the example code and include comments where you tell which words are close and which not. \n",
    "\n",
    "**Do not use the words *water* or *man* that have been supplied.** Also, don't change the window size or dimensionality for this task.\n",
    "\n",
    "Read on to **3.2** after you have done this part of the task.\n",
    "\n",
    "**Note:** *Use your best judgment on what \"relatively close\" means. It is hard to find words that have completely identical embeddings or words that have embeddings that are on completely opposite regions of the vector space. If you run the code cell below, you can see two word visualized, \"water\" and \"man\". \"Water\" is a good example of dissimilar embeddings and \"man\" is an example of a word with two similar embeddings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = \"water man\".split()\n",
    "\n",
    "distribsem.plot_two_embeddings(\n",
    "    words=words,\n",
    "    embeddings_1=M_moby_shared,\n",
    "    embeddings_2=M_sense_shared,\n",
    "    mapping_1=mapping_shared,\n",
    "    embeddings_1_name=\"Moby Dick\",\n",
    "    embeddings_2_name=\"Sense and Sensibility\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2: Your second task** is to pick **two** words you found in **3.1**: one with similar embeddings and one with dissimilar embeddings. Analyze the contexts of the two words in the two texts, similar to what you did in Task 2. How do the (dis)similarities of the embeddings show in the word contexts in different texts? Again, answer the question in your code. \n",
    "\n",
    "You do not need to change the window size of dimensionality in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n_occurrences = 10\n",
    "word = \"water\"\n",
    "\n",
    "print(\"Occurrences in Sense and Sensibility:\")\n",
    "distribsem.show_kwic(\n",
    "    text=sense_and_sensibility,\n",
    "    word=word,\n",
    "    window=4,\n",
    "    dimensionality=1000,\n",
    "    show_n=show_n_occurrences\n",
    ")\n",
    "\n",
    "print(\"\\nOccurrences in Moby Dick:\")\n",
    "distribsem.show_kwic(\n",
    "    text=moby_dick,\n",
    "    word=word,\n",
    "    window=4,\n",
    "    dimensionality=1000,\n",
    "    show_n=show_n_occurrences\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are down, download this page as a Python file (.py) and submit it through Moodle before the deadline. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
