{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 4, Part 3\n",
    "\n",
    "---\n",
    "\n",
    "In the lecture on distributional semantics, we learned about the **cosine similarity** measure. The equation for cosine similarity is the following: \n",
    "$$\n",
    "\\cos(v,w)\n",
    "= \\frac{ v \\cdot w }{ \\lvert v \\rvert \\lvert w \\rvert }\n",
    "= \\frac{ \\sum_{i=1}^{N} v_i w_i }{ \\sqrt{\\sum_{i=1}^{N} v_i^2} \\sqrt{\\sum_{i=1}^{N} w_i^2} }\n",
    "$$\n",
    "\n",
    "In this Part we will implement a function that calculates the cosine similarity between two vectors using NumPy, the Python library we worked with last time. If high school math isn't fresh on your mind, the equation might look scary. Don't worry, however; this part will walk you through it step-by-step.\n",
    "\n",
    "Let's first import numpy and then get right into it.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1: Dot product\n",
    "---\n",
    "Let's start by investigating the numerator in the equation:\n",
    "\n",
    "$v \\cdot w = \\sum_{i=1}^{N} v_i w_i$\n",
    "\n",
    "This is called a **dot product**. In plain English the equation says: Multiply the vectors $v$ and $w$ element-wise and sum the elements of the resulting vector. Let's calculate a dot product in two steps for with some actual values.\n",
    "\n",
    "---\n",
    "\n",
    "**Vectors:** \n",
    "\n",
    "$i = \\begin{bmatrix}2\\\\1\\\\3\\end{bmatrix}$, $j = \\begin{bmatrix}3\\\\-1\\\\0\\end{bmatrix}$ \n",
    "\n",
    "**Element-wise multiplication:**\n",
    "\n",
    "$i \\circ j \n",
    "= \\begin{bmatrix}2\\\\1\\\\3\\end{bmatrix} \\circ \\begin{bmatrix}3\\\\-1\\\\0\\end{bmatrix} \n",
    "= \\begin{bmatrix}2\\times3\\\\1\\times(-1)\\\\3\\times0\\end{bmatrix}\n",
    "= \\begin{bmatrix}6\\\\-1\\\\0\\end{bmatrix}$\n",
    "\n",
    "**Summing the values in the resulting vector:**\n",
    "\n",
    "$6 + (-1) + 0 = 5$\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3.1.1** Implement the function `dot` in the code cell below. Recall from last session that element-wise multiplication of two vectors is done using the operator `*`; the same operator we have used for normal multiplication. \n",
    "\n",
    "For computing the sums of the vector elements, NumPy offers a convenient function `np.sum()`:\n",
    "\n",
    "    >>> v = np.array([1, 2, 3])\n",
    "    >>> np.sum(v)\n",
    "    6\n",
    "\n",
    "In three steps:\n",
    "\n",
    "1. Element-wise multiply the vectors v and w. You can assign the result to a variable called `mul_result`, for example.\n",
    "2. Sum the elements of the resulting vector. Do this using `np.sum`. For example: `np.sum(mul_result)`.\n",
    "3. Return the sum.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"Compute the dot product of the vectors v and w.\"\"\"\n",
    "    return -1.0\n",
    "\n",
    "\n",
    "# FOR TESTING\n",
    "\n",
    "v = np.array([2, 1, 3])\n",
    "w = np.array([3, -1, 0])\n",
    "print(\"Result:\", dot(v, w)) # This should be 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2: The length of a vector\n",
    "----\n",
    "\n",
    "Now that we have the dot product out of the way, we can move on to the denumerator in the original equation.\n",
    "\n",
    "$\\lvert v \\rvert \\lvert w \\rvert = \\sqrt{\\sum_{i=1}^{N} v_i^2} \\sqrt{\\sum_{i=1}^{N} w_i^2}$\n",
    "\n",
    "$\\lvert v \\rvert$ is the length of the vector $v$, so in the equation above we multiply the lengths of the two vectors. Let's see how we can compute the length (also called **L2-norm**) of a single vector:\n",
    "\n",
    "$\\lvert v \\rvert = \\sqrt{\\sum_{i=1}^{N} v_i^2}$\n",
    "\n",
    "Again in plain English the equation says: square each element in the vector $v$, sum the elements of the resulting vector, and take the square root of the sum. Let's once again illustrate this with some actual values.\n",
    "\n",
    "---\n",
    "\n",
    "**Vector:** \n",
    "\n",
    "$i = \\begin{bmatrix}2\\\\1\\\\3\\end{bmatrix}$\n",
    "\n",
    "**Vector squared:**\n",
    "\n",
    "$\\begin{bmatrix}2^2\\\\1^2\\\\3^2\\end{bmatrix}\n",
    "= \\begin{bmatrix}4\\\\1\\\\9\\end{bmatrix}$\n",
    "\n",
    "**Summing:**\n",
    "\n",
    "$4 + 1 + 9 = 14$\n",
    "\n",
    "**Square root:**\n",
    "\n",
    "$\\sqrt{14} = 3.741...$\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1.2.1** Implement the function `l2norm` in the code cell below.\n",
    "\n",
    "You can square each element of a vector using the operator `**`:\n",
    "\n",
    "    >>> v = np.array([1, 2, 3])\n",
    "    >>> v**2\n",
    "    array([1, 4, 9])\n",
    "    \n",
    "Summing is done as in exercise 1.1.1. For calculating the square root, you can use the NumPy function `np.sqrt()`:\n",
    "\n",
    "    >>> np.sqrt(16)\n",
    "    4.0\n",
    "\n",
    "In four steps:\n",
    "\n",
    "1. Square each element in the vector v.\n",
    "2. Sum the elements of the resulting vector. Do this using `np.sum().\n",
    "3. Compute the square root of the sum using `np.sqrt()`.\n",
    "4. Return the resulting value.\n",
    "\n",
    "*Note: Feel free to use the function `dot` we implemented above if you can see the connection here. If you can't, no worries. Just follow the steps above.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(v):\n",
    "    \"\"\"Calculate the length (l2-norm) of the vector v.\"\"\"\n",
    "    return 0\n",
    "\n",
    "\n",
    "# FOR TESTING\n",
    "\n",
    "a = np.array([2, 1, 3])\n",
    "print(l2norm(a)) # This should be 3.74165..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3: Putting it all together\n",
    "---\n",
    "We now have all the necessary building blocks for computing the cosine similarity.\n",
    "\n",
    "$\n",
    "\\cos(\\vec{v},\\vec{w})\n",
    "= \\frac{ v \\cdot w }{ \\lvert v \\rvert \\lvert w \\rvert }\n",
    "$\n",
    "\n",
    "We can calculate the dot product $v \\cdot w$ with our function `dot` and the length of a vector $\\lvert v \\rvert$ with our function `l2norm`. Implementing a function for cosine similarity is now straightforward.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 1.3.1** Implement the function `cosine_similarity` below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v, w):\n",
    "    \"\"\"Compute the cosine similarity between the vectors v and w.\"\"\"\n",
    "    return 0\n",
    "\n",
    "\n",
    "# FOR TESTING\n",
    "\n",
    "v = np.array([1, 2, 3])\n",
    "w = np.array([1, 2, 3])\n",
    "\n",
    "print(cosine_similarity(v, w)) # This should be 1.0\n",
    "\n",
    "v = np.array([1, 2, 3])\n",
    "w = np.array([-1, -2, -3])\n",
    "\n",
    "print(cosine_similarity(v, w)) # This should be -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have completed all class assignments, you can continue with the home assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
