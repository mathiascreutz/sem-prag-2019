{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Pragmatics, KIK-LG103\n",
    "\n",
    "## Lab session 4, Part 2\n",
    "\n",
    "---\n",
    "\n",
    "We will now move on to the second main topic of the day, **clustering**. We will try out two different clustering methods that were introduced in the lecture. The first one is **k-means** and the second one is **hierarchical (agglomerative)** clustering.\n",
    "\n",
    "Once again, import all the necessary stuff before you read on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab4utils import embed, to_feature_matrix, \\\n",
    "                      plot_dendrogram, plot_kmeans, \\\n",
    "                      get_clusters_at_cutoff\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 2.1\n",
    "\n",
    "---\n",
    "\n",
    "The first method we wil look at is a \"flat\" clustering algorithm called k-means. Flat in this case means that the resulting clusters do not have any explicit structure. The optimal result is simply that words within a cluster are maximally similar to each other, while words in different clusters are maximally different from each other. We saw a [demo](http://shabal.in/visuals/kmeans/1.html) of how k-means clustering works; if you need a refresher you can check that out again.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2.1.1** In the code cell below we show you how to cluster a set of words and plot the results. The things you need to worry about are on the second and the third line. \n",
    "\n",
    "Try out different words and numbers of clusters and think about the following questions: \n",
    "\n",
    "- How well does the clustering work?\n",
    "- Which words seem to work best?\n",
    "- What kind of categories do you think the clusters represent?\n",
    "- Is there a number of clusters that gives sensible results most of the time, or one that doesn't work at all?\n",
    "- Do you see any problems with having to define the number of clusters yourself?\n",
    "- Do the clusters change when you run the algorithm several times?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the words to be clustered and plotted\n",
    "words = \"run jump swim walk go take cry laugh speak talk hear\".split()\n",
    "clusters = 2\n",
    "\n",
    "# Represent the words in a suitable way for the clustering algorithm\n",
    "X = to_feature_matrix(words)\n",
    "# Initialize clustering algorithm\n",
    "model = KMeans(n_clusters=clusters)\n",
    "# Train model\n",
    "model = model.fit(X)\n",
    "    \n",
    "# Plot results\n",
    "plot_kmeans(model, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Section 2.2\n",
    "\n",
    "---\n",
    "\n",
    "in this section we will look at the second method: **hierarchical agglomerative clustering**. The method is hierarchical because it gives us a hierarchy of clusters instead of the flat, structureless clusters of k-means. We can investigate the hierarchy at different depths, resulting in different clusters depending on the level where we decide to group the words. Agglomerative means that the algorithm works in a bottom-up manner. Initially, each word is considered its own cluster. The clusters are then iteratively merged until we end up with one cluster. This results in the hierarchical structure.\n",
    "\n",
    "All of this is easier to see in a dendrogram. Run the code cell below to see the results.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2.2.1** Again, try out different words. This time the number of clusters isn't the most important thing. As you might notice, the number you define doesn't change the structure of the resulting hierachy. What it changes is the depth where the algorithm groups the words into clusters. These clusters are shown after the word labels (`word/cluster_id`). Think about the following questions:\n",
    "\n",
    "- Do you see any potential benefits to using a hierarchical clustering instead of a flat one like k-means? Any problems? \n",
    "- Do the resulting clusters from this method match on to the clusters produced by k-means?\n",
    "- Do the clusters change when you run the algorithm multiple times?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the words to be clustered and plotted\n",
    "words = \"run jump swim walk go take cry laugh speak talk hear\".split()\n",
    "clusters = 4\n",
    "\n",
    "# Represent the words in a suitable way for the clustering algorithm\n",
    "X = to_feature_matrix(words)\n",
    "# Initialize clustering algorithm\n",
    "model = AgglomerativeClustering(n_clusters=clusters)\n",
    "model = model.fit(X)\n",
    "\n",
    "# Train model\n",
    "model = model.fit(X)\n",
    "    \n",
    "# Plot results\n",
    "plot_dendrogram(model, labels=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 2.2.2** In this exercise you have a chance to make sure you understand how the final clusters can be determined from the dendrogram. \n",
    "\n",
    "In the code cell below you are given a function `get_clusters_at_cutoff`, that takes the clustering `model`, the source words and a cutoff value as arguments. It will print out the clusters that we would get if we cut off the hierarchy at the depth determined by the value *(Note: Depth in this case is calculated \"bottom-up\")*. \n",
    "\n",
    "In the example code we use a cutoff value 9. In that case all the words end up in a single cluster.\n",
    "\n",
    "Now make sure you understand how that is determined. Look at the dendrogram above, and try to predict what kind of clusters form at a certain depth. Then use the function below to check if you predicted correctly. \n",
    "\n",
    "Try this with a few different sets of words and cutoff values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clusters_at_cutoff(model, words, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can continue to Part 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
